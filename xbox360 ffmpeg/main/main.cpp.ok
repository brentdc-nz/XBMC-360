// main.cpp : Defines the entry point for the application.
//
#include "stdafx.h"
#define UINT64_C(val) val##ui64
extern "C"{
#include <avcodec.h>
#include <avformat.h>
#include <swscale.h>
}

#undef printf
#undef fprintf
#undef malloc
#undef free

#include <stdio.h>
#include "shader.h"

#define SAFE_DELETE(a) { delete (a); (a) = NULL; }

//-------------------------------------------------------------------------------------
// Structure to hold vertex data.
//-------------------------------------------------------------------------------------
struct COLORVERTEX
{
    float       Position[3];
    float       Coor[2];
};

//-------------------------------------------------------------------------------------
// Global variables
//-------------------------------------------------------------------------------------
D3DDevice*             g_pd3dDevice;    // Our rendering device
D3DVertexBuffer*       g_pVB;           // Buffer to hold vertices
D3DVertexDeclaration*  g_pVertexDecl;   // Vertex format decl
D3DVertexShader*       g_pVertexShader; // Vertex Shader
D3DPixelShader*        g_pPixelShader;  // Pixel Shader

IDirect3DTexture9 * g_pFrameU = NULL;
IDirect3DTexture9 * g_pFrameV = NULL;
IDirect3DTexture9 * g_pFrameY = NULL;

XMMATRIX g_matWorld;
XMMATRIX g_matProj;
XMMATRIX g_matView;

BOOL g_bWidescreen = TRUE;

int video_ffmepg(char *);

//-------------------------------------------------------------------------------------
// Name: InitD3D()
// Desc: Initializes Direct3D
//-------------------------------------------------------------------------------------
HRESULT InitD3D()
{
    // Create the D3D object.
    Direct3D* pD3D = Direct3DCreate9( D3D_SDK_VERSION );
    if( !pD3D )
        return E_FAIL;

    // Set up the structure used to create the D3DDevice.
    D3DPRESENT_PARAMETERS d3dpp; 
    ZeroMemory( &d3dpp, sizeof(d3dpp) );
    XVIDEO_MODE VideoMode;
    XGetVideoMode( &VideoMode );
    g_bWidescreen = VideoMode.fIsWideScreen;
    d3dpp.BackBufferWidth        = min( VideoMode.dwDisplayWidth, 1280 );
    d3dpp.BackBufferHeight       = min( VideoMode.dwDisplayHeight, 720 );
    d3dpp.BackBufferFormat       = D3DFMT_X8R8G8B8;
    d3dpp.BackBufferCount        = 1;
    d3dpp.EnableAutoDepthStencil = TRUE;
    d3dpp.AutoDepthStencilFormat = D3DFMT_D24S8;
    d3dpp.SwapEffect             = D3DSWAPEFFECT_DISCARD;
    d3dpp.PresentationInterval   = D3DPRESENT_INTERVAL_ONE;

    // Create the Direct3D device.
    if( FAILED( pD3D->CreateDevice( 0, D3DDEVTYPE_HAL, NULL,
                                    D3DCREATE_HARDWARE_VERTEXPROCESSING,
                                    &d3dpp, &g_pd3dDevice ) ) )
        return E_FAIL;

    return S_OK;
}


//-------------------------------------------------------------------------------------
// Name: InitScene()
// Desc: Creates the scene.  First we compile our shaders. For the final version
//       of a game, you should store the shaders in binary form; don't call 
//       D3DXCompileShader at runtime. Next, we declare the format of our 
//       vertices, and then create a vertex buffer. The vertex buffer is basically
//       just a chunk of memory that holds vertices. After creating it, we must 
//       Lock()/Unlock() it to fill it. Finally, we set up our world, projection,
//       and view matrices.
//-------------------------------------------------------------------------------------
HRESULT InitScene()
{
    // Compile vertex shader.
    ID3DXBuffer* pVertexShaderCode;
    ID3DXBuffer* pVertexErrorMsg;
    HRESULT hr = D3DXCompileShader( g_strVertexShaderProgram, 
                                    (UINT)strlen( g_strVertexShaderProgram ),
                                    NULL, 
                                    NULL, 
                                    "main", 
                                    "vs_2_0", 
                                    0, 
                                    &pVertexShaderCode, 
                                    &pVertexErrorMsg, 
                                    NULL );
    if( FAILED(hr) )
    {
        if( pVertexErrorMsg )
            OutputDebugString( (char*)pVertexErrorMsg->GetBufferPointer() );
        return E_FAIL;
    }    

    // Create vertex shader.
    g_pd3dDevice->CreateVertexShader( (DWORD*)pVertexShaderCode->GetBufferPointer(), 
                                      &g_pVertexShader );

    // Compile pixel shader.
    ID3DXBuffer* pPixelShaderCode;
    ID3DXBuffer* pPixelErrorMsg;
    hr = D3DXCompileShader( g_strPixelShaderProgram, 
                            (UINT)strlen( g_strPixelShaderProgram ),
                            NULL, 
                            NULL, 
                            "main", 
                            "ps_2_0", 
                            0, 
                            &pPixelShaderCode, 
                            &pPixelErrorMsg,
                            NULL );
    if( FAILED(hr) )
    {
        if( pPixelErrorMsg )
            OutputDebugString( (char*)pPixelErrorMsg->GetBufferPointer() );
        return E_FAIL;
    }

    // Create pixel shader.
    g_pd3dDevice->CreatePixelShader( (DWORD*)pPixelShaderCode->GetBufferPointer(), 
                                     &g_pPixelShader );
    
    // Define the vertex elements and
    // Create a vertex declaration from the element descriptions.
    D3DVERTEXELEMENT9 VertexElements[3] =
    {
        { 0,  0, D3DDECLTYPE_FLOAT3, D3DDECLMETHOD_DEFAULT, D3DDECLUSAGE_POSITION, 0 },
        { 0, 12, D3DDECLTYPE_FLOAT2, D3DDECLMETHOD_DEFAULT, D3DDECLUSAGE_TEXCOORD, 0 },
        D3DDECL_END()
    };
    g_pd3dDevice->CreateVertexDeclaration( VertexElements, &g_pVertexDecl );

    // Create the vertex buffer. Here we are allocating enough memory
    // (from the default pool) to hold all our 3 custom vertices. 
    if( FAILED( g_pd3dDevice->CreateVertexBuffer( 4*sizeof(COLORVERTEX),
                                                  D3DUSAGE_WRITEONLY, 
                                                  NULL,
                                                  D3DPOOL_MANAGED, 
                                                  &g_pVB, 
                                                  NULL ) ) )
        return E_FAIL;

    // Now we fill the vertex buffer. To do this, we need to Lock() the VB to
    // gain access to the vertices. This mechanism is required because the
    // vertex buffer may still be in use by the GPU. This can happen if the
    // CPU gets ahead of the GPU. The GPU could still be rendering the previous
    // frame.
    COLORVERTEX g_Vertices[] =
    {
        //square
		{ -1.0f, -1.0f, 0.0f,  0.0f,  1.0f },//1
		{ -1.0f,  1.0f, 0.0f,  0.0f,  0.0f },//2
		{  1.0f,  1.0f, 0.0f,  1.0f,  0.0f },//4
		{  1.0f, -1.0f, 0.0f,  1.0f,  1.0f }//3
    };


    COLORVERTEX* pVertices;
    if( FAILED( g_pVB->Lock( 0, 0, (void**)&pVertices, 0 ) ) )
        return E_FAIL;
    memcpy( pVertices, g_Vertices, 4*sizeof(COLORVERTEX) );
    g_pVB->Unlock();

    // Initialize the world matrix
    g_matWorld = XMMatrixIdentity();

    // Initialize the projection matrix
    //FLOAT fAspect = ( g_bWidescreen ) ? (16.0f / 9.0f) : (4.0f / 3.0f); 
	FLOAT fAspect = 1.0f; 
    g_matProj = XMMatrixPerspectiveFovLH( XM_PIDIV4, fAspect, 1.0f, 200.0f );

    // Initialize the view matrix
    XMVECTOR vEyePt    = { 0.0f, 0.0f,-3.0f, 0.0f };
    XMVECTOR vLookatPt = { 0.0f, 0.0f, 0.0f, 0.0f };
    XMVECTOR vUp       = { 0.0f, 1.0f, 0.0f, 0.0f };
    g_matView = XMMatrixLookAtLH( vEyePt, vLookatPt, vUp );

    return S_OK;
}

//-------------------------------------------------------------------------------------
// Name: Update()
// Desc: Updates the world for the next frame
//-------------------------------------------------------------------------------------
void Update()
{
    // Set the world matrix
    // float fAngle = fmodf( -g_Time.fAppTime, XM_2PI );
    // static const XMVECTOR vAxisZ = { 0, 1.0f, 0.0f, 0 };
    // g_matWorld = XMMatrixRotationAxis( vAxisZ, fAngle );
}


//-------------------------------------------------------------------------------------
// Name: Render()
// Desc: Draws the scene
//-------------------------------------------------------------------------------------
void Render()
{
    // Clear the backbuffer to a blue color
    g_pd3dDevice->Clear( 0L, NULL, D3DCLEAR_TARGET|D3DCLEAR_ZBUFFER|D3DCLEAR_STENCIL, 
                         D3DCOLOR_XRGB(0,0,255), 1.0f, 0L );

    // Draw the triangles in the vertex buffer. This is broken into a few steps:
    
    // We are passing the vertices down a "stream", so first we need
    // to specify the source of that stream, which is our vertex buffer. 
    // Then we need to let D3D know what vertex and pixel shaders to use. 
    g_pd3dDevice->SetVertexDeclaration( g_pVertexDecl );
    g_pd3dDevice->SetStreamSource( 0, g_pVB, 0, sizeof(COLORVERTEX) );
    g_pd3dDevice->SetVertexShader( g_pVertexShader );
    g_pd3dDevice->SetPixelShader( g_pPixelShader );

	// set YUV texture ...
	g_pd3dDevice->SetTexture(0,g_pFrameY);
	g_pd3dDevice->SetTexture(1,g_pFrameU);
	g_pd3dDevice->SetTexture(2,g_pFrameV);
   
    // Build the world-view-projection matrix and pass it into the vertex shader
    XMMATRIX matWVP = g_matWorld * g_matView * g_matProj;
    g_pd3dDevice->SetVertexShaderConstantF( 0, (FLOAT*)&matWVP, 4 );

    // Draw the vertices in the vertex buffer
	g_pd3dDevice->DrawPrimitive( D3DPT_RECTLIST, 0,1 );

    // Present the backbuffer contents to the display
    g_pd3dDevice->Present( NULL, NULL, NULL, NULL );

	g_pd3dDevice->SetTexture(0,NULL);
	g_pd3dDevice->SetTexture(1,NULL);
	g_pd3dDevice->SetTexture(2,NULL);
}

void InitYuvSurface(int width, int height){

	D3DXCreateTexture(
		g_pd3dDevice, width,
		height, D3DX_DEFAULT, 0, D3DFMT_LIN_L8, D3DPOOL_MANAGED,
		&g_pFrameY
	);
	D3DXCreateTexture(
		g_pd3dDevice, width/2,
		height/2, D3DX_DEFAULT, 0, D3DFMT_LIN_L8, D3DPOOL_MANAGED,
		&g_pFrameU
	);
	D3DXCreateTexture(
		g_pd3dDevice, width/2,
		height/2, D3DX_DEFAULT, 0, D3DFMT_LIN_L8, D3DPOOL_MANAGED,
		&g_pFrameV
	);

};

//-------------------------------------------------------------------------------------
// Name: main()
// Desc: The application's entry point
//-------------------------------------------------------------------------------------
void __cdecl main()
{
    // Initialize Direct3D
    if( FAILED( InitD3D() ) )
        return;

    // Initialize the vertex buffer
    if( FAILED( InitScene() ) )
        return;

	video_ffmepg("game:\\video.avi");
}

void displayFrame(AVPicture * pict,int w, int h){
	
	RECT d3dr;
	d3dr.left=0;
	d3dr.top=0;
	d3dr.right=w;
	d3dr.bottom=h;

    // Update the world
    Update();   
    // Render the scene
    Render();
}

#include <xaudio2.h>

WAVEFORMATEX * g_pWfx;
AVPacket   g_AudioPacket;

int videoStreamNumber=-1;
int audioStreamNumber=-1;
AVCodec    *pAudioCodec;
AVCodecContext  *pAudioCodecCtx;

IXAudio2* g_pXAudio2 = NULL;
IXAudio2MasteringVoice* g_pMasteringVoice = NULL;
IXAudio2SourceVoice* g_pSourceVoice = NULL;

XAUDIO2_BUFFER g_SoundBuffer;

HRESULT InitAudio(){
//-------------------------------------------------------------------------------------
// Initialise Audio
//-------------------------------------------------------------------------------------	
	HRESULT hr;
	if( FAILED( hr = XAudio2Create( &g_pXAudio2, 0 ) ) )
    {
		OutputDebugStringA( "Failed to init XAudio2 engine\n");
        return E_FAIL;
    }

//-------------------------------------------------------------------------------------
// Create a mastering voice
//-------------------------------------------------------------------------------------	
    if( FAILED( hr = g_pXAudio2->CreateMasteringVoice( &g_pMasteringVoice ) ) )
    {
		OutputDebugStringA( "Failed creating mastering voice\n");
        return E_FAIL;
    }
//-------------------------------------------------------------------------------------
// Create source voice
//-------------------------------------------------------------------------------------	
	WAVEFORMATEXTENSIBLE wfx;
	memset(&wfx, 0, sizeof(WAVEFORMATEXTENSIBLE));

	//memset(&g_AudioPacket,0,sizeof g_AudioPacket);
/*
	g_pWfx->nChannels=pAudioCodecCtx->channels;
	g_pWfx->nSamplesPerSec=pAudioCodecCtx->sample_rate;
    g_pWfx->wBitsPerSample=16;
    g_pWfx->wFormatTag=WAVE_FORMAT_PCM;
    g_pWfx->nBlockAlign=g_pWfx->nChannels*16/8;
    g_pWfx->nAvgBytesPerSec=g_pWfx->nSamplesPerSec*g_pWfx->nBlockAlign;
*/
	wfx.Format.wFormatTag           = WAVE_FORMAT_EXTENSIBLE ;
	wfx.Format.nSamplesPerSec       = pAudioCodecCtx->sample_rate;//48000 by default
	wfx.Format.nChannels            = pAudioCodecCtx->channels;
	wfx.Format.wBitsPerSample       = 16;
	wfx.Format.nBlockAlign          = wfx.Format.nChannels*16/8;
	wfx.Format.nAvgBytesPerSec      = wfx.Format.nSamplesPerSec * wfx.Format.nBlockAlign;
	wfx.Format.cbSize               = sizeof(WAVEFORMATEXTENSIBLE)-sizeof(WAVEFORMATEX);
	wfx.Samples.wValidBitsPerSample = wfx.Format.wBitsPerSample;
	wfx.dwChannelMask               = SPEAKER_STEREO;
	wfx.SubFormat                   = KSDATAFORMAT_SUBTYPE_PCM;

//-------------------------------------------------------------------------------------
//	Source voice
//-------------------------------------------------------------------------------------
	if(FAILED( g_pXAudio2->CreateSourceVoice(&g_pSourceVoice,(WAVEFORMATEX*)&wfx, XAUDIO2_VOICE_NOSRC | XAUDIO2_VOICE_NOPITCH , 1.0f, NULL)	))
	{
		OutputDebugStringA("CreateSourceVoice failed\n");
		return E_FAIL;
	}
	
//-------------------------------------------------------------------------------------
// Start sound
//-------------------------------------------------------------------------------------	
	if ( FAILED(g_pSourceVoice->Start( 0 ) ) )
	{
		OutputDebugStringA("g_pSourceVoice failed\n");
		return E_FAIL;
	}
}

void UpdateAudio(){

}

int resizeArray( BYTE *&_array,int fromSize,int toSize )
{
	BYTE * newptr=(BYTE*)malloc(toSize * sizeof(BYTE)) ;//new BYTE[toSize];
    memcpy(newptr,_array,toSize);
    free(_array);
    _array=newptr;
    return 0;
}

void UpdateFrame(SwsContext *pCtx, AVFrame * pFrame, int width, int height){
	AVPicture pict;
	memset(&pict, 0, sizeof(pict));

	D3DLOCKED_RECT lockRectY;
	D3DLOCKED_RECT lockRectU;
	D3DLOCKED_RECT lockRectV;

	g_pFrameU->LockRect( 0, &lockRectY, NULL, 0 );
	g_pFrameV->LockRect( 0, &lockRectU, NULL, 0 );
	g_pFrameY->LockRect( 0, &lockRectV, NULL, 0 );

	pict.data[2] = (uint8_t*)lockRectY.pBits;
	pict.data[1] = (uint8_t*)lockRectU.pBits;
	pict.data[0] = (uint8_t*)lockRectV.pBits;

	pict.linesize[2] = lockRectY.Pitch;
	pict.linesize[1] = lockRectU.Pitch;
	pict.linesize[0] = lockRectV.Pitch;

	sws_scale(pCtx, pFrame->data, pFrame->linesize, 0, height, pict.data, pict.linesize);

	g_pFrameU->UnlockRect(0);
	g_pFrameY->UnlockRect(0);
	g_pFrameV->UnlockRect(0);

	displayFrame(&pict,width, height);
}

int video_ffmepg(char * v)
{
	AVFormatContext *pFormatCtx;
	int             i;
	AVCodecContext  *pCodecCtx;
	AVCodec         *pCodec;
	AVFrame         *pFrame; 
	AVPacket        packet;
	int             frameFinished;
	int             numBytes;

	SwsContext * pSWSContext;

	// Register all formats and codecs
	av_register_all();

	// Open video file
	if(av_open_input_file(&pFormatCtx, v, NULL, 0, NULL)!=0)
		return -1; // Couldn't open file

	// Retrieve stream information
	if(av_find_stream_info(pFormatCtx)<0)
		return -1; // Couldn't find stream information

	// Dump information about file onto standard error
	dump_format(pFormatCtx, 0, v, 0);


	// Find the first video stream
	for(i=0; i<pFormatCtx->nb_streams; i++){
		if(pFormatCtx->streams[i]->codec->codec_type==AVMEDIA_TYPE_VIDEO) {
			videoStreamNumber=i;
		}
		else if(pFormatCtx->streams[i]->codec->codec_type==AVMEDIA_TYPE_AUDIO) {
			audioStreamNumber=i;
		}
	}

	
		if(videoStreamNumber==-1)
		{
			OutputDebugStringA("Didn't find a video stream!\n");
			return -1; // Didn't find a video stream
		}

		//Audio
		if(audioStreamNumber>0)
		{
			pAudioCodecCtx=pFormatCtx->streams[audioStreamNumber]->codec;

			// Find the decoder for the audio stream
			pAudioCodec=avcodec_find_decoder(pAudioCodecCtx->codec_id);
			if(pAudioCodec==NULL) {
				OutputDebugStringA("Unsupported codec!\n");
				return -1; // Codec not found
			}
			
			// Inform the codec that we can handle truncated bitstreams -- i.e.,
			// bitstreams where frame boundaries can fall in the middle of packets
			if(pAudioCodec->capabilities & CODEC_CAP_TRUNCATED)
				pAudioCodecCtx->flags|=CODEC_FLAG_TRUNCATED;

			// Open codec
			if(avcodec_open(pAudioCodecCtx, pAudioCodec)<0)
			{
				OutputDebugStringA("Open codec!\n");
				return -1; // Could not open codec
			}

			InitAudio();
		}

		//Video
		{
			// Get a pointer to the codec context for the video stream
			pCodecCtx=pFormatCtx->streams[videoStreamNumber]->codec;

			// Find the decoder for the video stream
			pCodec=avcodec_find_decoder(pCodecCtx->codec_id);
			if(pCodec==NULL) {
				OutputDebugStringA("Unsupported codec!\n");
				return -1; // Codec not found
			}
			
			// Inform the codec that we can handle truncated bitstreams -- i.e.,
			// bitstreams where frame boundaries can fall in the middle of packets
			if(pCodec->capabilities & CODEC_CAP_TRUNCATED)
				pCodecCtx->flags|=CODEC_FLAG_TRUNCATED;

			// Open codec
			if(avcodec_open(pCodecCtx, pCodec)<0)
			{
				OutputDebugStringA("Open codec!\n");
				return -1; // Could not open codec
			}

		}
		// Allocate video frame
		pFrame=avcodec_alloc_frame();

		//YUV
		InitYuvSurface(pCodecCtx->width, pCodecCtx->height);
	
		pSWSContext = sws_getContext(pCodecCtx->width, pCodecCtx->height, pCodecCtx->pix_fmt, pCodecCtx->width, pCodecCtx->height, PIX_FMT_YUV420P, SWS_BILINEAR, 0, 0, 0);

		BYTE * AudioBytes = (BYTE *)malloc(AVCODEC_MAX_AUDIO_FRAME_SIZE*sizeof(BYTE));//new BYTE[AVCODEC_MAX_AUDIO_FRAME_SIZE];

		while(av_read_frame(pFormatCtx, &packet)>=0) {
			// Is this a packet from the video stream?
			if(packet.stream_index==videoStreamNumber) {
				// Decode video frame
				int len =avcodec_decode_video(pCodecCtx, pFrame, &frameFinished, 
					packet.data, packet.size);

				// Did we get a video frame?
				if(frameFinished) {
					UpdateFrame(pSWSContext, pFrame,pCodecCtx->width, pCodecCtx->height);
				}
			}
			else if(packet.stream_index==audioStreamNumber){
				g_SoundBuffer.pAudioData=AudioBytes;
				int done=AVCODEC_MAX_AUDIO_FRAME_SIZE;

				if (avcodec_decode_audio2(
					pAudioCodecCtx,(int16_t *)g_SoundBuffer.pAudioData,&done,
					packet.data,packet.size
					) < 0 ) 
				{
					OutputDebugString("glitch while decoding\n");
                     av_free_packet(&packet);
                     continue;
                }
                if (done) {
                    g_SoundBuffer.AudioBytes=done;
					//g_SoundBuffer.AudioBytes=1200;
					//resizeArray((BYTE *&)g_SoundBuffer.pAudioData,AVCODEC_MAX_AUDIO_FRAME_SIZE,done);
                    //av_free_packet(&g_AudioPacket);
					g_pSourceVoice->FlushSourceBuffers();
                    //-------------------------------------------------------------------------------------
					// Send sound stream
					//-------------------------------------------------------------------------------------	
					if( FAILED(g_pSourceVoice->SubmitSourceBuffer( &g_SoundBuffer ) ) )
					{
						OutputDebugStringA("SubmitSourceBuffer failed\n");
					}
					// Sleep(8);
                }
			}

			// Free the packet that was allocated by av_read_frame
			av_free_packet(&packet);
		}

		// Free the YUV frame
		if(pFrame)
			av_free(pFrame);

		// Close the codec
		avcodec_close(pCodecCtx);

		// Close the video file
		av_close_input_file(pFormatCtx);

		return 0;
}
